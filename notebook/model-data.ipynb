{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/gbemidebe/Documents/GitHub/SolutionTransformer/notebook\n",
      "Changed working directory: /Users/gbemidebe/Documents/GitHub/SolutionTransformer\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "os.chdir('../')\n",
    "print(f'Changed working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20870, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solute</th>\n",
       "      <th>solvent</th>\n",
       "      <th>T</th>\n",
       "      <th>log_gamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>CCCCCCCCCCCCCCCC</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-0.261365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>CCCCCCCCCCCCCCCC</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>CCCCCCCCCCCCCCCC</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-0.301105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC</td>\n",
       "      <td>CCCCCCCCCCCCCCCC</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-0.235722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC</td>\n",
       "      <td>CCCCCCCCCCCCCCCC</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-0.248461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  solute           solvent     T  log_gamma\n",
       "0      C  CCCCCCCCCCCCCCCC  40.0  -0.261365\n",
       "1      C  CCCCCCCCCCCCCCCC  70.0  -0.287682\n",
       "2      C  CCCCCCCCCCCCCCCC  90.0  -0.301105\n",
       "3     CC  CCCCCCCCCCCCCCCC  40.0  -0.235722\n",
       "4     CC  CCCCCCCCCCCCCCCC  70.0  -0.248461"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/processed/cleaned_Brouwer_2021.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data: (16696, 4)\n",
      "Shape of test data: (4174, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SmilesData:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def get_split(self, train_ratio=0.8, seed=None):\n",
    "        n = len(self.data)\n",
    "        indices = np.arange(n)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "        train_size = int(train_ratio * n)\n",
    "        train_indices = indices[:train_size]\n",
    "        test_indices = indices[train_size:]\n",
    "        train_data = self.data.iloc[train_indices].reset_index(drop=True)\n",
    "        test_data = self.data.iloc[test_indices].reset_index(drop=True)\n",
    "        return train_data, test_data\n",
    "\n",
    "train_data, test_data = SmilesData(data).get_split(seed=2024)\n",
    "print(f'Shape of train data: {train_data.shape}')\n",
    "print(f'Shape of test data: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbemidebe/miniconda3/envs/SolutionTransformer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the tokenizer\n",
    "model_name = \"seyonec/PubChem10M_SMILES_BPE_450k\" \n",
    "#\"DeepChem/ChemBERTa-77M-MTR\" #'seyonec/PubChem10M_SMILES_BPE_450k'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class Input(Dataset):\n",
    "    def __init__(self, data, tokenizer, column_names = ['solute', 'solvent', 'T', 'log_gamma']):\n",
    "        '''\n",
    "        data: pandas dataframe with columns \"solute\", \"solvent\", \"T\", \"log_gamma\n",
    "        tokenizer: tokenizer to use'''\n",
    "\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.column_names = column_names\n",
    "        self.max_length = self.max_len(column_names[0]) + self.max_len(column_names[1]) + 3 # The total character length including [CLS], [SEP], and [PAD]\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Returns the length of the dataset'''\n",
    "        return len(self.data)\n",
    "\n",
    "    def max_len(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the maximum length of the input sequence\n",
    "        \"\"\"\n",
    "        return max([len(x) for x in self.data[idx]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the input data as a dictionary with keys \"solute\", \"solvent\", \"input_ids\", \"attention_mask\", \"Temp\", and \"label\"\n",
    "        \"\"\"\n",
    "        solute = self.data.iloc[idx][self.column_names[0]]\n",
    "        solvent = self.data.iloc[idx][self.column_names[0]]\n",
    "        inputs = self.tokenizer(solute, solvent, return_tensors=\"pt\", padding='max_length', truncation=True,\n",
    "                                max_length=self.max_length)\n",
    "        \n",
    "        sample = {\n",
    "                    'solute': solute,\n",
    "                    'solvent': solvent,\n",
    "                    'input_ids': inputs[\"input_ids\"].squeeze(0),\n",
    "                    'attention_mask': inputs[\"attention_mask\"].squeeze(0),\n",
    "                    'Temp': torch.tensor(self.data.iloc[idx][self.column_names[2]], dtype=torch.float).unsqueeze(0),\n",
    "                    'label': torch.tensor(self.data.iloc[idx][self.column_names[3]], dtype=torch.float).unsqueeze(0)\n",
    "                    }\n",
    "        return sample\n",
    "# create the train and test datasets\n",
    "TrainData = Input(train_data, tokenizer)\n",
    "TestData = Input(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 16696\n",
      "Length of test data: 4174\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of train data: {len(TrainData)}')\n",
    "print(f'Length of test data: {len(TestData)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solute': 'CC(C)CC(C)(C)C',\n",
       " 'solvent': 'CC(C)CC(C)(C)C',\n",
       " 'input_ids': tensor([  0, 262,  12,  39,  13, 262,  12,  39, 274,  39,  13,  39,   2,   2,\n",
       "         262,  12,  39,  13, 262,  12,  39, 274,  39,  13,  39,   2,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'Temp': tensor([60.]),\n",
       " 'label': tensor([-0.1755])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainData[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"seyonec/PubChem10M_SMILES_BPE_450k\" \n",
    "# configuration\n",
    "config = AutoConfig.from_pretrained(model_name,num_labels=1)\n",
    "# load in the model\n",
    "BaseModel = AutoModelForSequenceClassification.from_pretrained(model_name, config=config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaClassificationHead\n",
    "\n",
    "class SmilesTransformerModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 model_name = \"seyonec/PubChem10M_SMILES_BPE_450k\", # model name\n",
    "                 n_classes=1):\n",
    "        super(SmilesTransformerModel, self).__init__() \n",
    "        # pretrained model\n",
    "        config = AutoConfig.from_pretrained(model_name,return_dict = False) # configuration\n",
    "        self.transformer = RobertaModel.from_pretrained(model_name, config=config,) # load in the model\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        # self.fc1 = nn.Linear(self.transformer.config.hidden_size, self.transformer.config.hidden_size)\n",
    "        # self.fc2 = nn.Linear(self.transformer.config.hidden_size, self.transformer.config.hidden_size)\n",
    "        self.fc3 = nn.Linear(self.transformer.config.hidden_size, n_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None, temperature: torch.Tensor = None, labels: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        :param input_ids: SMILES encodings\n",
    "        :param attention_mask: attention mask (1 for non-padding token and 0 for padding)\n",
    "        :param temperature: temperature\n",
    "        \"\"\"\n",
    "\n",
    "        _, transformer_outputs = self.transformer(input_ids = input_ids,\n",
    "                                    attention_mask = attention_mask)\n",
    "        #sequence_output = transformer_outputs[0]\n",
    "        #print('transformer_outputs', sequence_output.shape)\n",
    "\n",
    "        drop_out = self.drop(transformer_outputs)\n",
    "        # fc1 = F.relu(self.fc1(drop_out))\n",
    "        # fc2 = F.relu(self.fc2(fc1))\n",
    "        #print('drop_out', drop_out.shape)\n",
    "        output = self.fc3(drop_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# loss_fn = nn.MSELoss()\n",
    "# smilesmodel = SmilesTransformerModel()\n",
    "# train_loader = torch.utils.data.DataLoader(TrainData, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# for batch in train_loader:\n",
    "#     print('input_ids: ', batch['input_ids'].shape, 'attention_mask: ', batch['attention_mask'].shape, 'Temp: ', batch['Temp'].shape, 'label: ', batch['label'].shape)\n",
    "#     out = smilesmodel(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], temperature=batch['Temp'], labels=batch['label'])\n",
    "#     print('out', out.shape)\n",
    "#     print('loss: ', loss_fn(out, batch['label']))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class ModelTrainer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "        ************** Train/Test the model using cross validation ************** \n",
    "        seed: seed for random number generator\n",
    "        epochs: number of epochs to train\n",
    "        lr: learning rate\n",
    "        train: flag whether to train the model\n",
    "        log_interval: how many batches to wait before logging training status\n",
    "        model: takes input_ids: str, attention_mask: str, classification: bool\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed = 2023, lr=2e-5, epochs=1000):\n",
    "        super(ModelTrainer, self).__init__()\n",
    "        self.seed = seed \n",
    "        self.epochs = epochs \n",
    "        self.lr = lr  \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "        self.model = SmilesTransformerModel().to(self.device)\n",
    "         \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "     \n",
    "        self.loss_func = nn.MSELoss()  \n",
    "\n",
    "    def validate(self, val_loader, model, device, loss_func):\n",
    "        \"\"\"Evaluate the network on the entire validation (part of training data) set.\"\"\"\n",
    "\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for data in val_loader:\n",
    "                # get the inputs\n",
    "                input_ids = data['input_ids'].to(device)\n",
    "                input_mask = data['attention_mask'].to(device)\n",
    "                labels = data['label'].to(device)\n",
    "                # forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "                # loss and accuracy\n",
    "                loss = loss_func(outputs, labels)\n",
    "                val_loss += loss.sum().item() * input_ids.size(0)\n",
    "    \n",
    "\n",
    "        return val_loss, val_accuracy\n",
    "\n",
    "    def test(self, test_loader, model, loss_func, device):\n",
    "        \"\"\"Evaluate the network on the entire test set and calculate AUC.\"\"\"\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        test_loss, test_accuracy = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                # get the inputs\n",
    "                input_ids = data['input_ids'].to(device)\n",
    "                input_mask = data['attention_mask'].to(device)\n",
    "                labels = data['label'].to(device)\n",
    "\n",
    "                # forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "                # loss and accuracy\n",
    "                loss = loss_func(input=outputs, target=labels)\n",
    "                test_loss += loss.sum().item() * input_ids.size(0)\n",
    "\n",
    "        return test_loss, test_accuracy          \n",
    "    \n",
    "\n",
    "    def train(self, model, train_loader, loss_func, optimizer, device):\n",
    "        \"\"\"Train the network on the training set.\"\"\"\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            \n",
    "            # get the inputs\n",
    "            input_ids = data['input_ids'].to(device)   # amino acid index numbers\n",
    "            input_mask = data['attention_mask'].to(device) # attention mask (1 for non-padding token and 0 for padding)\n",
    "            labels = data['label'].to(device) # True for classification task\n",
    "            # forward pass\n",
    "            outputs = self.model(input_ids = input_ids, attention_mask = input_mask)\n",
    "            # loss and backward pass\n",
    "            loss = loss_func(input=outputs, target=labels)\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # loss and accuracy\n",
    "            train_loss += loss.sum().item() * input_ids.size(0)\n",
    "\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "    def execute_run(self, train_loader, test_loader, fold = 3, batch_size = 32):\n",
    "        '''Train, Test and Validate the network on the training set using cross validation.'''\n",
    "\n",
    "        print(f\"Training on: {self.device}\")\n",
    "\n",
    "        torch.manual_seed(self.seed) # set the seed for generating random numbers\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        \n",
    "        # split data for K-fold cross validation to avoid overfitting\n",
    "        self.fold = fold\n",
    "        indices = list(range(len(train_loader.dataset)))\n",
    "        kf = KFold(n_splits=self.fold, shuffle=True)\n",
    "\n",
    "        for cv_index, (train_indices, valid_indices) in enumerate(kf.split(indices)):\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "            train_loader = DataLoader(train_loader.dataset, batch_size=batch_size,\n",
    "                                                       sampler=train_sampler,\n",
    "                                                       shuffle=False, pin_memory=True)\n",
    "            val_loader = DataLoader(train_loader.dataset, batch_size=batch_size,\n",
    "                                                     sampler=valid_sampler,\n",
    "                                                     shuffle=False, pin_memory=True)\n",
    "            \n",
    "            print(\"CV: {}\".format(cv_index))\n",
    "\n",
    "            self.history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "            for epoch in range(0, self.epochs + 1):\n",
    "                # Training\n",
    "                epoch_train_loss, epoch_train_accuracy = self.train(model=self.model, \n",
    "                                                                    train_loader=train_loader, loss_func=self.loss_func, \n",
    "                                                                    optimizer=self.optimizer, device=self.device)\n",
    "                # Validation\n",
    "                epoch_val_loss, epoch_val_accuracy = self.validate(val_loader=val_loader, \n",
    "                                                                    model=self.model, loss_func=self.loss_func, \n",
    "                                                                    device=self.device)\n",
    "                # \n",
    "                train_loss = epoch_train_loss / len(train_loader.sampler)\n",
    "                val_loss = epoch_val_loss / len(val_loader.sampler)\n",
    "\n",
    "                self.history['train_loss'].append(train_loss)    \n",
    "                self.history['val_loss'].append(val_loss)\n",
    "         \n",
    "\n",
    "                # train & validation error after every epoch\n",
    "                print(\"Epoch: {}/{}, Training Loss: {:.4f}, Validation Loss: {:.4f}\".format(\n",
    "                                epoch, self.epochs, train_loss, val_loss))\n",
    "            # after cross validation\n",
    "            print(f'Finished training & validation the model for CV..... {cv_index} .........')\n",
    "            avg_train_loss_after_CV = np.mean(self.history['train_loss'])\n",
    "            avg_val_loss_after_CV = np.mean(self.history['val_loss'])\n",
    "            \n",
    "            print(\"Average Training Loss: {:.4f} \\t Average Val Loss: {:.4f}\".format(avg_train_loss_after_CV, avg_val_loss_after_CV))  \n",
    "            \n",
    "        # model testing\n",
    "        print('Testing the model...')\n",
    "        test_loss, test_accuracy = self.test(test_loader=test_loader, model=self.model, loss_func=self.loss_func, device=self.device)\n",
    "        test_loss_ = test_loss / len(test_loader.sampler)\n",
    "\n",
    "        print(\"Test Loss: {:.4f}\".format(test_loss_))\n",
    "        print('Finished training & testing the model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(TrainData, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(TestData, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# model\n",
    "model = ModelTrainer()\n",
    "model.execute_run(train_loader=train_loader, test_loader=test_loader, fold=3, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
